\documentclass[12pt]{report}
%\documentclass[12pt]{report}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry
\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{titlesec}
\usepackage{color}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{framed}
\usepackage{soul}

% See the ``Article customise'' template for come common customisations
\newcommand{\note}[1]{{\color{blue} {#1}}}

\addtolength{\topmargin}{-1.5cm}
\addtolength{\textheight}{2.5cm}

\author{Anna Golubeva}

%%% BEGIN DOCUMENT
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   IB -- toy dataset
\begin{center}
{\bf\Large{Computing Mutual Information (MI)}}\\
%\vspace{5mm}
%(notes on subtask 2 from our project plan)
\end{center}
\noindent\rule{\textwidth}{0.4pt}

The MI between two random variables $A$ and $B$ distributed according to $p(a,b)$ is defined as:
\begin{align}
I(A;B) &= \sum \limits_{a} \sum \limits_{b} p(a,b) \log\frac{p(a,b)}{p(a)p(b)},
\end{align}
where $p(a)$ and $p(b)$ are the marginal distributions of $A$ and $B$, respectively, defined as $p(a) = \sum_y p(a|b)p(b)$ (and vice versa).

We are looking to compute the quantities $I(X;T)$ and $I(T;Y)$, that is the MI between the input $X$ or label $Y$ and a representation of the input given by a hidden layer $T$, respectively.
\begin{itemize}
\item input: $X=x$,
\item[] $x\in [0,1]^n$, $n=784$, $|X| = 60000$ in the MNIST dataset
\item[] $x\in \{0,1\}^n$, $n=12$, $|X| = 4096$ in the toy dataset
\item label: $Y=y$,
\item[] $y\in \{0,1\}^n$, $n = |Y| = 10$ in the MNIST dataset
\item[] $y\in \{0,1\}^n$, $n= |Y| = 2$ in the toy dataset (binary classification)
\item hidden layer (input representation): $T=t$,
\item[] $t\in\mathbb{R}^n$, $n=\#$ units in the layer, i.e. depends on the network architecture and is i.g. different for each hidden layer
\end{itemize}

\vspace{1cm}
{\bf Remarks on the range of values that $x$, $y$ or $t$ can take}

The values of $x$ and $y$ are given by the dataset. In case of MNIST, the inputs are greyscale images of size $28\times 28$ pixels, each pixel taking any value from the {\it continuous} interval between 0 and 1 (0 for the color white, 1 for black). The images display handwritten digits, and the learning task is to classify the images according to the 10 classes corresponding to digits 0-9. The label to each image is encoded as a ``1-hot vector'', that is a vector of length 10, with only the element corresponding to the assigned digit being non-zero. Example: If an image $x$ displays the digit 3, the corresponding label is $y = (0,0,0,1,0,0,0,0,0,0)$.

In case of the toy dataset, the inputs are binary vectors of length 12, i.e. each element can take the value 0 or 1 (note the different brackets, square vs. curly, in the notation above!). The classification is binary, i.e. the labels only take two possible values (0 and 1). Thus, the ``1-hot'' vector $y$ has length 2 in this case.
 
The possible values of $t$ depend on the network architecture in two ways: The number of elements in the vector is given by the number of units in the layer, and the possible values of each element depend on the activation function of the respective units. For {\it sigmoid} and {\it tanh} functions, the output is a continuous number on the interval $[0,1]$ and $[-1,1]$, respectively.

The network architecture for the toy dataset is described in section 3 of the paper: ``up to'' 7 fully connected hidden layers, with 12-10-7-5-4-3-2 neurons, {\bf hyperbolic tangent} function as activation for all neurons except for the final layer, {\bf sigmoidal} activation for the final layer.

For calculations on the MNIST dataset, the authors unfortunately do not specify any details. The only mention of it can be found in the paper on page 17: {\it ``[...] we examined the the SG statistics for a standard large problem (MNIST classification) and found the transition to the SGD diffusion phase.''} We assume for now that they use the standard architecture from the \href{https://www.tensorflow.org/get_started/mnist/beginners}{Tensorflow tutorial}.


\vspace{1cm}
{\bf Computing MI on the toy dataset}

Section 3.2 of the paper provides the following information:

Each hidden layer of the network is treated as a singe random variable, denoted as $T$ (or $T_i$, but the layer index $i$ will be omitted to simplify notation).
In order to compute the MI, we need to estimate the probability distributions $p(x,t)=p(t|x)p(x)$ and $p(t,y)=p(t|y)p(y)$.

Since all configurations are equally probable, the distribution for $X$ is uniform:
\begin{equation}
p(x) = \frac{1}{4096} = \text{const.} \equiv p_x.
\end{equation}

The outputs of each neuron are discretized by binning into 30 equal intervals between -1 and 1. These discretized values are used to directly calculate $P(X, T_i)$ and $P(T_i, Y) = \sum_x P(x,Y)P(T_i | x)$.

For the following discussion we will slightly adjust the notation, using the following conventions: The input, label and each hidden layer are described by the random variables $X$, $Y$ and $T$, respectively. The lower-case letters $x$, $y$ and $t$ denote the instances of the random variable, i.e. the possible configurations (of the input or the hidden layer) or the possible labels. The probability functions will be denoted by lower-case $p$.

The MI between $X$ and $T$ is given by
\begin{align}
I(X;T) &= \sum \limits_{x} \sum \limits_{t} p(x,t) \log\frac{p(x,t)}{p(x)p(t)}.
\end{align}
Using the chain rule, the joint probability of $x$ and $t$ can be expressed through the conditional $p(t|x)$:
\begin{equation}
p(x,t) = p(t|x)p(x) = p(t|x) p_x.
\end{equation}
The number of all possible configurations of $T$ is $|T| = 30^n$, where $n$ is the number of units in the layer, and 30 is the number of possible values each unit can take, given by the number of bins. Given a network (fixed set of weights and biases, denoted collectively as $\omega$), a configuration $x$ will be mapped to a representation $t$. Since $t$ is a coarsened representation of $x$, it is likely that several different configurations $x$ will be mapped onto the same representation $t$. However, the conditional probability for a representation $t$ given a configuration $x$ is either 0 or 1, i.e. a delta function:
\begin{equation}
p(t|x) = \delta_{t,t^{(\omega)}_x}
\end{equation}
where $t^{(\omega)}_x$ denotes the representation $t$ for a given $x$ that is obtained from the network which is specified by a set of parameters $\omega$. For the sake of readability, $\omega$ will be omitted in the following. With this, the MI can be expressed as
\begin{align}
I(X;T) &= \sum \limits_{x} \sum \limits_{t} p(t|x) p(x) \log\frac{p(t|x)p(x)}{p(x)p(t)}\\
   &= p_x \sum \limits_{x} \sum \limits_{t} \delta_{t,t_x} \log\frac{\delta_{t,t_x}}{p(t)}\\
   &= p_x \sum \limits_{x} \log\frac{1}{p(t_x)}\\
   &= - p_x \sum \limits_{x} \log p(t_x)   
\end{align}
and
\begin{align}
I(T;Y) &= \sum \limits_{y} \sum \limits_{t} p(y,t) \log\frac{p(y,t)}{p(y)p(t)}\\
&= \sum \limits_{y} \sum \limits_{t} \sum \limits_{x} p(x,y) p(t|x) \log\frac{ \sum \limits_{x'} p(x',y) p(t|x') }{p(y)p(t)}\\
&= \sum \limits_{y} \sum \limits_{t} \sum \limits_{x} p(x,y) \delta_{t,t_x} \log\frac{ \sum \limits_{x'} p(x',y) \delta_{t,t_{x'}} }{p(y)p(t)}\\
&= \sum \limits_{y} \sum \limits_{x} p(x,y) \log\frac{ \sum \limits_{x'} p(x',y) \delta_{t_x,t_{x'}} }{p(y)p(t_x)}\\
&= p_x \sum \limits_{y} \sum \limits_{x} p(y|x) \log\frac{ p_x \sum \limits_{x'} p(y|x') \delta_{t_x,t_{x'}} }{p(y)p(t_x)}. \label{eq:here}
\end{align}

Note that the remaining Kronecker delta in Eq.~\eqref{eq:here} restricts the sum in the argument of the log to only those configurations $x'$ with $t_{x'}=t_x$, where $x$ is set by the sum outside of the log.


The marginal probability of the label is
\begin{equation}
p(y) = \sum\limits_x p(y|x)p(x) = p_x \sum\limits_x p(y|x),
\end{equation}
and the conditional probability for $y$ given $x$ was defined in the construction of the toy dataset:
\begin{equation}
p(y=1|x) = \frac{1}{1+e^{-\gamma (f(x)-\theta)}},
\end{equation}
with parameters $\gamma$ and $\theta$ that are tuned such that $p(y=1|x) \approx 0.5$ and $I(X;Y) \approx 0.99 \text{ bits}$.

Thus, the remaining quantity required to calculate the MIs is the marginal probability of a representation $t_x$ of a given $x$, $p(t_x)$. The representation $t_x$ is given by the outputs of the layer's units, each binned into 30 equal intervals between $-1$ and $1$. Let $n_T$ denote the number of units in the layer $T$. The output of this layer for a given configuration $x$ can be written as a vector of length $n_T$, it's elements being the bin numbers of each unit's output. The binning is effectively a rounding of the decimal outputs, performed to enable identification of equal representations. $p(t_x)$ for the given layer $T$ is obtained by collecting the $t_x$ for each $x$ and dividing the number of their occurences by the total number of configurations (which is 4096 for this dataset). Note that the order of the elements matters, i.e. $t_x = (14, 5, 27) \neq (27, 14, 5) = t_{x'}$.

Notes on the algorithmic implementation in python (TensorFlow):

The representations $t_x~\forall~x$ are obtained by retrieving the network parameters in each epoch and calculating the network outputs for each configuration $x$ in the dataset. The binning is performed by the numpy function $digitize$, with bins defined through $linspace(-1,1,31)$. The numpy function $unique$ is used to remove duplicate $t$ from the array. It provides an array of unique elements $t$, ordered, as well as an array of indices 

%unique_elems, indices, counts = np.unique(t_vectors, return_counts=True, return_inverse=True, axis=0)
%unique_elems # list of elements from input t_vectors, duplicates removed, ordered
%counts # number of occurences for each element in the unique_elems
%indices # indices of counts to each element in input t_vectors, i.e. "count of representation t_vectors[i]" = counts[indices[i]]

The Kronecker delta in Eq.~\eqref{eq:here} is implemented as a correspondence matrix using the array indices. On the construction logic: For each $x$ given by the outside sum, we are interested in finding all $x'$ with the same representation, i.e. $t_{x'} = t_x$. Thus, for configuration $x=configs[k]$, the indices of all $x'$ fulfilling $t_{x'} = t_x$ can be obtained through $t_vectors==t_vectors[k]$. Note that this operation involves comparing vectors; there is a computationally cheaper operation which will yield the same result: $indices==indices[k]$.
indices[k] holds the index for the correct element of the array count. The values in indices are the same for equal $t$.



\end{document}